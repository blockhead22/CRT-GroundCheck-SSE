# User Research: Paraphrasing Upgrade Validation

> **ðŸš€ Quick Start:** New to this project? Start with [`KICKOFF_SUMMARY.md`](KICKOFF_SUMMARY.md) then run `python kickoff_user_research.py`  
> **ðŸ“š Full Navigation:** See [`INDEX.md`](INDEX.md) for complete documentation index

## Overview

This directory contains the framework for conducting user research to validate the real-world impact of improved paraphrasing accuracy (85-90%) delivered by the hybrid neural + semantic matcher upgrade in GroundCheck (PR #18).

## Objective

Begin user research to validate the real-world impact of improved paraphrasing accuracy (85-90%) delivered by the hybrid neural + semantic matcher upgrade in GroundCheck.

## Success Metrics

- Clear, quantified user feedback on value of paraphrasing improvements
- Recommendations for future model & matcher tuning

## Directory Structure

```
user_research/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ benchmarking/                      # User benchmarking experiments
â”‚   â”œâ”€â”€ experiment_protocol.md         # Experiment design & protocol
â”‚   â””â”€â”€ groundingbench_tasks.md        # Specific GroundingBench tasks
â”œâ”€â”€ error_collection/                  # Sample error collection
â”‚   â”œâ”€â”€ collection_framework.md        # Framework for collecting errors
â”‚   â””â”€â”€ error_templates.md             # Templates for documenting errors
â”œâ”€â”€ interviews/                        # User interview materials
â”‚   â”œâ”€â”€ interview_guide.md             # Interview questions & guide
â”‚   â””â”€â”€ participant_consent.md         # Consent form template
â”œâ”€â”€ analysis/                          # Pre/post-upgrade analysis
â”‚   â”œâ”€â”€ error_classification.md        # Error classification framework
â”‚   â””â”€â”€ analysis_protocol.md           # Analysis methodology
â””â”€â”€ findings/                          # Research findings & recommendations
    â”œâ”€â”€ findings_template.md           # Template for documenting findings
    â””â”€â”€ recommendations.md             # Next iteration recommendations
```

## Getting Started

### Option 1: Automated Setup (Recommended)
```bash
cd user_research
python kickoff_user_research.py
```
This will:
- Create data collection directories
- Generate tracking files and templates
- Provide a quick start guide
- Set up your research environment

### Option 2: Manual Setup
1. Review the [experiment protocol](benchmarking/experiment_protocol.md)
2. Set up GroundingBench tasks from [groundingbench_tasks.md](benchmarking/groundingbench_tasks.md)
3. Use the [error collection framework](error_collection/collection_framework.md) to gather sample errors
4. Conduct interviews using the [interview guide](interviews/interview_guide.md)
5. Analyze results using the [error classification](analysis/error_classification.md) framework
6. Document findings in [findings_template.md](findings/findings_template.md)

### Complete Documentation
See **[INDEX.md](INDEX.md)** for:
- Complete navigation of all documents
- Quick start workflows
- Common questions and answers
- Document dependencies

## Related Documentation

- [GROUNDCHECK_IMPROVEMENTS.md](../GROUNDCHECK_IMPROVEMENTS.md) - Technical details of PR #18
- [GroundingBench Data](../groundingbench/data/) - Benchmark datasets
- [Experiment Scripts](../experiments/) - Evaluation scripts

## Timeline

**Phase 1: Setup (Week 1)**
- Finalize experiment design
- Recruit participants
- Prepare GroundingBench tasks

**Phase 2: Data Collection (Weeks 2-3)**
- Conduct benchmarking experiments
- Collect error samples
- Perform user interviews

**Phase 3: Analysis (Week 4)**
- Classify errors pre/post-upgrade
- Analyze interview transcripts
- Synthesize findings

**Phase 4: Reporting (Week 5)**
- Document quantified user feedback
- Create recommendations for future tuning
- Present findings to stakeholders

## Contact

For questions or feedback, please create an issue in the repository.
